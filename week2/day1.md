# 第一天6.24
## 经典的统计方法（机器学习算法）研究

了解各种技术背后的想法是非常重要的，以便知道如何以及何时使用它们。
*区分统计学与机器学习*

- 机器学习是人工智能的一个子领域。
- 统计学习是统计学的一个分支。
- 机器学习更强调大规模应用和预测的准确性。
- 统计学习强调模型及其可解释性，精确性和不确定性。但是这种区别变得越来越模糊，并且存在着很多“交叉”。
- 机器学习在市场营销中占据上风！

### 线性回归 
一元线性回归和多元线性回归，这部分内容较为熟悉，对其数学原理及实现与应用已基本掌握，不用再过多学习。就是针对线性回归模型，怎么防止过拟合，这里再做个梳理。

*特征缩减技术*

其中这里不仅仅是针对线性回归模型，通过对损失函数(即优化目标)加入惩罚项（加入正则项），使得训练求解参数过程中会考虑到系数的大小，通过设置缩减系数(惩罚系数)，会使得影响较小的特征的系数衰减到0，只保留重要的特征。常用的缩减系数方法有lasso(L1正则化)，岭回归(L2正则化)。

*特征缩减技术的目的*
- 消除噪声特征：消除一些不必要的特征，提升模型准确率；
- 消除关联特征：如果模型空间存在具有关联的特征，那么会使得模型不够稳定；

其中L1正则化对相关特征的消除无能为力。然后在机器学习算法参数的选择上，通常是通过使用*交叉验证*来确定最优参数。

### 重采样方法
重采样是从原始数据中重复采集样本的方法。这是一种非参数统计推断方法。换句话说，重采样方法不涉及使用通用分布表来计算近似的p概率值。

重采样根据实际数据生成一个唯一的采样分布。它使用实验方法而不是分析方法来生成唯一的样本分布。它产生的是无偏估计，因为它是基于研究人员研究的数据的所有可能结果生成的无偏样本。为了理解重采样的概念，需要理解术语Bootstrapping和交叉验证(Cross-Validation)。

- Bootstrapping ：在很多情况下是一种有用的方法，比如评估模型性能、模型集成(ensemble methods)、估计模型的偏差和方差等。它的工作机制是对原始数据进行有放回的采样，并将“没被选上”的数据点作为测试用例。我们可以这样操作多次，并计算平均得分作为模型性能的估计。

- 交叉验证： 是评估模型性能的一种方法，它通过将训练数据分成k份，使用k-1份作为训练集，使用保留的那份作为测试集。以不同的方式重复整个过程k次。最终取k个得分的平均值作为模型性能的估计。

### logistic regression (LR模型)
logistic回归是一种二分类算法（不是回归算法），对于多分类问题则要进行多次LR模型。主要就是应用sigmoid函数，损失函数的构造，以及加入正则项，最小化模型的求解。
笔记本已有记录，数学原理及实现与应用也已掌握。

### 支持向量机（SVM）
支持向量机是一种分类算法，在LR模型上的损失函数进行了改进，由于SVM的良好性质，得到了广泛的应用，属于机器学习中的监督学习模型。通俗地说，它通过寻找超平面（二维中的线，三维中的平面和更高维中的超平面，更正式地，超平面是n维空间的n-1维子空间）以及最大边界（margin）来划分两类点。从本质上讲，它是一个约束优化问题，因为其边界最大化受到数据点分布的约束（硬边界）。
相应的还有SVR模型（支持向量回归模型）。

经典的分类算法还有朴素贝叶斯算法等

### 降维算法
- PCA
- LDA
- TSNE（最新降维算法）

### 无监督的分类（聚类）
聚类算法是无监督的分类，不知道数据的标签，根据数据间的相似关系进行分类。可以分为：
- 基于划分的聚类算法
- 基于层次的聚类算法
- 基于密度的聚类算法等

后续学习过程：基于树的方法---决策树、随机深林等



